\chapter{Theoretical background}
\label{chapter:background}

In this chapter, we introduce the necessary theoretical background as well as
explain the reasons why the current work is important given the latest
developments in the theory of distributed computing. Unless specified otherwise,
the material presented in this chapter is based on Hirvonen 2020~\cite{Hirvonen2020}.

\section{Graphs}

This section will outline the necessary theoretical background on graphs. 
Most of the notation related graph theoretic concepts in the current
section and in the rest of the thesis will follow that of
a recent introductory textbook on distributed algorithms~\cite{Hirvonen2020},
unless specified otherwise.

A \emph{graph} is a pair $G = (V, E)$, where $V$ denotes the set of all \emph{vertices}
and $E$ denotes a set of all \emph{edges}. Each edge in $E$ is represented as a set of
2 nodes i.e. the nodes the given edge is connecting e.g. $e = \{v, u\}$ such that 
$v \in V$ and $u \in V$.

When talking about the \emph{size of a graph}, or \emph{cardinality} of the graph, we refer
to the number of nodes in the graph. Or in other words, the size of a graph $G$
is always $|V|$.

Among other things, edges can be categorised into \emph{directed} and \emph{undirected}.
The latter ones simply connect a pair of nodes in a graph, while the latter
ones also contain extra information about which of the 2 connected nodes is
a "source" and which one is a "destination". Informally, such a directed edge
can be visualised as an error that starts in a "source" node and ends in a 
"destination" node.  Similarly, we talk about \emph{undirected graphs} - that is graphs
where all edges are undirected, and \emph{directed graphs} - such graphs where all
edges are directed. Formally, while an undirected edge $e$ is defined as a
set of 2 nodes $e = \{v, u\}$, a directed edge $e$ is defined as a set tuple
of 2 nodes $e = (v, u)$. Recall that in the case of a tuple, the order of its
elements does matter, hence, the tuple allows us to encode the direction of
the edge. Unless mentioned otherwise, we assume that an edge is directed
from the first element of the tuple to the second. Also note that in this
thesis, unless specified explicitly, all graphs are assumed to be undirected.

If two nodes are connected by an edge, we call such nodes \emph{neighbours}. We
also say that such nodes are \emph{adjacent} to each other. Moreover, in an undirected graph
if there are two edges $e_1$ and $e_2$ such that $e_1 \neq e_2$ and $e_1 \cap e_2 \neq \emptyset$
such edges are said to be \emph{adjacent} to each other. Similarly, we can talk about adjacent 
edges in a directed graph. In that case, two edges $e_1$ and $e_2$ are adjacent if
$e_1 \neq e_2$ and either first or second element of tuple $e_1$ contains the same
element as either first or second element of tuple $e_2$.

Besides we often talk about edges and nodes being \emph{incident} to each other.
In an undirected graph, an edge $e$ is incident to a node $v$ if and only if $v \in e$. In this
case, we also say that the node $v$ is incident to the edge $e$. Similarly, in a
directed graph, an edge $e$ is incident to a node $v$ if and only if $v$ is the
first or the second element of the tuple $e$. Again, in the case described in the
previous sentence, the node $v$ is said to be incident to the edge $e$.

A \emph{simple graph} is an undirected graph where no 2 nodes are connected by more
than one edge and no edge starts and ends at the same node. In other words, there are
no multiple edges or self-loops in a simple graph.

A \emph{degree} of a node $v \in V$ in a graph $G = (V, E)$ is the number of all
edges incident to $v$. That is it is the number of edges such that for an edge
$e$, $v \in e$ if $e$ is an undirect edge, or $v$ is the first or second element of
tuple $e$ if $e$ is a directed edge. Moreover, for directed graphs, we often talk
about \emph{indegrees} and \emph{outdegrees}. Indegree of a node $v$ is the number
of edges incident to $v$ directed towards the node, while outdegree of a node $v$ is the 
number of edges incident to $v$ directed outwards from the node. Finally, we are 
often interested in a maximum degree of a graph $G$, that is the maximum value of
degrees of all nodes belonging to the graph $G$. We denote such maximum degree as
$\Delta$

A \emph{walk} in an undirected graph $G = (V, E)$ is a sequence $w$ of a form
$w = (v_0, e_1, v_1, e_2, ..., e_l, v_l)$,
where $v_i \in V$ and $e_i \in E$, and $e_i = {v_{i-1}, v_i}$ for all $i$. A walk from some
node $v$ to some other node $u$ is then a walk $w$ such that its first element i.e. $v_0 = v$
and its last element i.e. $v_l = u$. Having defined a walk, we can talk about a \emph{
connected graph}. A connected graph, when talking about undirected
graphs, is a graph $G = (V, E)$ in which for any pair of nodes $v$ and $u$ such that
$v \neq u$ there is a walk from $v$ to $u$.

An \emph{isomorphism} between two graphs $G_1$ and $G_2$ is is a function $f$ such that $f$ is a 
bijection, and $f$ maps a vertex of the graph $G_1$ to a vertex of the graph $G_2$, and 
an edge between some nodes $v$ and $u$ exists in the graph $G_1$ if and only if
an edge between nodes $f(v)$ and $f(u)$ exists in the graph $G_2$. From this,
it is rather easy to see that if an isomorphism exists from $G_1$ to $G_2$, then
also an isomorphism exists from $G_2$ to $G_1$. If there exists an
isomorphism between some two graphs, we say that such graphs are isomorphic
(to each other).

A \emph{radius-x neighbourhood} of a node $v$ in a graph $G = (V, E)$ is a set of
all such nodes $u \in V$ that there exists a walk between $v$ and $u$ and 
the shortest walk from $v$ to $u$ is at most $x$. Note that it is possible that
$v = u$, in which case the shortest walk between $v$ and $u$ is 0.

A diameter of a graph (denoted as $diam(G)$) is the length of the longest walk
in a list $W$, where $W$ is a list of the shortest walks - one between each pair of nodes
$v$ and $u$ in a graph $G = (V, E)$ such that $v \in V$ and $u \in V$. If a graph
$G$ is nont connected and therefore there is a pair of nodes $v$ and $u$ that do not
have a walk between them, we say that a diameter of such a graph $G$ is infinity. That
is $diam(G) = \inf$.

\subsection{Several important graph families}

This subsection will introduce some of the graph families that will be necessary
for understanding the content of the thesis. First, we'll define paths and cycles.
Then, we'll briefly introduce trees and explain the difference between rooted
and unrooted trees.

A \emph{path} graph is a sequence of nodes that are joined together by edges and
that have the following properties:

\begin{enumerate}
  \item The whole graph is connected. That is there is a walk from any node of
  the path to any other node of the path.

  \item The graph consists of at least 2 nodes.

  \item Exactly 2 nodes have a degree 1 and all the other nodes have a degree 2.
\end{enumerate}

A \emph{cycle} is a connected graph where each node has a degree 2.

A \emph{tree} is an undirected acyclic connected graph. A \emph{rooted tree}
has a single \emph{root vertex}, and thus some nodes have a \emph{parent} node
(some because e.g. a root node never has a parent) and one or more \emph{children}
nodes. For a node $v$, a node $u$ is its parent if and only if $v$ and $u$ are
neighbour nodes and the shortest walk from the root to $u$ is shorter by exactly
1 compared to the shortest walk from the root to $v$. Node $v$ is a child of node
$u$ if and only if $u$ is a parent of node $v$. Finally, in the context of rooted trees,
a node $l$ that has no children is referred to as a \emph{leaf} node.
On the other hand, an \emph{unrooted tree} has no single root, and therefore the notion of parent
or child is not defined. Instead, we talk about neighbours of some node $v$. However, we 
still use the notion of leaves to denote nodes of degree 1.

\section{Distributed computing}

Now that we have introduced some of the key graph theoretic concepts, we will
next outline some of the foundations of distributed computing. We'll start with
a short historical node about the field of distributed computing. Then, we'll
explain some of the aspects of the model of computation we're concerned with. Finally, 
we'll give some formal definitions to the model of computation that will be our
major concern throughout the rest of the thesis.

\subsection{General background}

The field of distributed computing studies computation in distributed systems, which
have become ubiquitous in the modern world, being especially prevalent
in the area of technology~\cite{Attiya2004}.
A distributed system consists of a number of relatively independent
computing modules, which usually need to cooperate in order to
fulfill a computational task the system has been given. Usually,
each computing module in such a system only has a part of the whole
input and is required to produce only a part of the whole output.
This is in contrast with a centralised system, in which there 
exists an all-knowing entity taking all of the input, performing all
of the computation and producing the whole of a result as its output.
Due to its modularised and parallel nature, distributed computing
has many applications in communication, computation, the Internet,
but also in biology and sociology~\cite{Wattenhofer2016}.

The field of distributed computing appeared already in late 1980s, with several
prominent papers exploring potentialities of computation performed
by multiple interconnected processing units~\cite{Cole1986, Linial1987, Naor1991}.
The first major step in the field happened in 1987, when Linial formalised
some of the principles of one variant of a distributed system.
This model of computation is currently known as Linial's or \emph{LOCAL model}~\cite{Linial1987}.

\subsection{Model of computation}

Here, we will describe the model of computation that we are going to
assume throughout the rest of the thesis. Note that there is a number
of different models, which are also widely studied in the research
community.

First, as it was already stated, unlike in the case of centralised
computation, we are concerned with multiple interconnected computing entities
that together form a graph. Each such an entity is referred to as a vertex or
a node in a graph. Connections between the vertices are referred to as edges.
The entities only can transfer information along the edges and in no other way
between each other. Unless specified otherwise, the graphs are assumed to be
simple graphs, and all edges are assumed to be undirected. Moreover,
communication along the edges can simultaneously happen in both direction.

Apart from sending infromation to each other, nodes can also
do local computation based on the local information each node possesses. One thing
to note that differs significantly from some of the centralised models of computation
is that each vertex in a graph has arbitrarily huge, but finite, storage and computation resources.
Informally, this means that anything that can be computed in a centralised setting
in a finite amount of time,
can also be computed locally by a node in an arbitrarily small amount of time.
It is also worth noticing that in the model of computation described here,
every node of a graph executed the same algorithm. Nevertheless, the
algorithm might lead to different commands being executed by different nodes
if, for example, nodes' unique identifiers or initial local inputs are different.
Besides, behaviour might also differ if two nodes have somewhat different neighbourhoods
around themselves and therefore will receive potentially different
information during their communication rounds. Finally, at the beginning of
execution, each node knows only its own input (including its own unique identifier
if such has been provided as part of the input) and its own degree i.e. the number of
its neighbour nodes in the graph.

Furthermore, computation in a graph happens in synchronous rounds. That implies,
for example, that round $x+1$ is not started by any of the nodes before all of the
nodes have completed round $x$. Moreover, each round is divided into three stages:

\begin{enumerate}
\item Sending some information to some (or all) of its neighbours

\item Receiving information
from some (or all) of its neighbours

\item Performing some local computaiton based
on the information that has been stored by a node locally before and the information
received during the current round from its neighbours

\item Updating its local state i.e. replacing and adding data in its own local storage
\end{enumerate}

Each of these stages is also executed
completely synchronously by all nodes in a graph, meaning that e.g. no node starts processing
any of its data, before all other nodes have received data from their neighbours. The computation
of a graph is said to be finished when all of the nodes have outputted their final states
and halted. More formally, this means that there are a set of node states that are
considered as "halting states". Whenever a node switches its internal state to one of the 
halting state, it does not perform any of the actions from that point in time, or - to
be even more formal - it does not send any messages to its neighbours, ignores all of the 
messages sent to it and does not update its internal state in all of the subsequent rounds.
Similar to the requirement that local computation of each node on each round has to be finite
and needs to stop after a finite amount of time passed, all nodes are required to eventually
transition into one of the halting states. That is a valid distributed algorithm - in our model of
computing - cannot continue for an infinite number of rounds. Finally,
the complexity of a distributed algorithm is measured as number of such synchronous rounds
of computation before the algorithm ends i.e. before all nodes transition into one of 
the halting states. Notice that different from a centralised model of computation,
algorithms complexity (or in other words its running time) is not affected by the
amount of local computation on a single node.

Another important thing that has to be mentioned when describing the mode of computation
is that all of the operations withing each individual node as well as inter-vertex
communication is absolutely error-free. In other words, all nodes can be assumed to
always act in a fault-free manner, all sent messages can be assumed to never be lost
or undelivered. Therefore, we can always assume that messages sent and received by nodes
are all in accordence to the actual algorithm that is being executed by the nodes 
and not a result of an accidental fault or an intentional adversary effort

\subsection{Distinctive qualities of LOCAL model}

As was already mentioned previously, our model of computation is known
as Linial's or \emph{LOCAL model}~\cite{Linial1987}. Therefore, to complete the
description, we will describe in more detail two distinctive qualities of LOCAL model
from other models of distributed computing, namely unique identifiers and arbitrarily large
bandwidth.

Each node in LOCAL model is provided with a unique identifier as part of initial input.
The identifiers are usually assumed to be integer numbers between 1 and $|V|^c$, where
$|V|$ is the number of nodes in the graph and $c$ is some constant. Unless specified, we
assume that such a constant $c$ is not known by the nodes at the beginning of
algorithm execution. Thus, unique identifiers are guaranteed to be positive integer
numbers bounded by a polynomial in the number of nodes, but it is not known - by the nodes
at the start of algorith execution - what is the largest unique identifier in the graph. Moreover,
the identifiers of the node do not necessarily form a continuous range of integers. That is
if an integer $x$ is used as a unique identifier for some node $v$, it is possible
that an integer $x+1$ is not used as an identifier in the graph. This implies that at
the beginning, nodes do not known what integers have been used for unique identifiers
and what have not been, with the exception of only one integer - their own identifier.

Another characteristic of LOCAL model, which also was already mentioned before, is the fact
that nodes can send (and receive) an arbitrarily large (but nevertheless finite)
amount of bytes of information over a single edge during one round. This fact,
combined with the existence of unique identifiers, renders the model as a rather strong one.
In particular, this implies that a node can send all of its information - no matter how
large it is - to all its neighbours in just one round.

This, in turn, implies a rather
curious property. Imagine that every node sends all of its information during the first round,
and having received some data from its neighbours, saves all this information locally. Consider
some node $v$ in the middle of a graph $G = (V, E)$. Since all nodes send all its data,
after the first round, node $v$
will have all the data that all its neighbours had initially, plus its own initial information.
Notice that each of $v$'s neighbours now have all the information of their neighbours. Thus,
if we combine all the data in posession of $v$ and $v$'s neighbours after the first round, it
is easy to observe that they together have all the information of $v$'s radius-2 neighbourhood.
But this means that after the second round, when all $v$'s neighbours have sent all their
information to $v$, $v$ alone posessses all the data that its radius-2 neighbourhood had at the
beginning of the algorithm execution. Similarly, after the 3rd round, $v$ will have all the
radius-3 neighbourhood's data, and so on.
In general, after round $x$, node $v$ will have
all information that was initailly available in its radius-$x$ neighbourhood.
Therefore, when considering LOCAl model,
time and space are in certain sense equivalent. In other words, the number of rounds
needed to solve a certain problem is always equal to the distance (or radius) to which a node needs to
see to solve a certain problem. One technicality to note here is that because all nodes have
unique identifiers, and these identifiers are sent together with the rest of data,
receiving nodes can differentiate what nodes the received data belongs to, and consequently,
reconstruct the structure of the neighbourhood in the graph.

As a consequence of the above,
we can observe that after $diam(G)$ rounds, node $v$ will have all the information there is in the 
graph $G$. This implies that after $diam(G)$ rounds, in LOCAL model, we can solve anything that can
be solved in a centralised setting. That is because at the end of $diam(G)$'th round, each node in the
graph have collected all the information there is in the graph and thus can just run the
computation locally. And because local computation in LOCAL model is virtually free,
anytihng that could be computed in a centralised setting will be computed locally by each
node individually. On the next round, each node can output their part of the solution.

\subsection{Formalising LOCAL model}

In this subsection we will formalise LOCAl model that we informally described above.
This will, first of all, help to clarify the aspects of the model that are still left ambiguous
after reading the previous subsections. Besides, it will allow us to introduce
a randomised model below once the basic deterministic one is unambiguously defined.

An algorithm being executed by each node consists of 
three functions. One for local state initialization that is executed only once, after
a node $v$ has received its initial local inputs $u(v)$ but before the first round

$$init_A(u(v), d)$$
where $A$ is a given distributed algorithm and $d$ is the degree of the node $v$.
The function return an initial local state of the node $v$.

The second function takes as an input an internal state $x(v)$ of a node $v$
and returns a tuple of size $d$, where $d$ is a degree of node $v$. The
tuple contains messages that are to be sent to $d$ neighbours of the node $v$
during the current round.

$$send_A(x(v), d)$$

The third function takes as an input a local internal state $x(v)$ of a node
$v$, and a tuple $m(v)$ of size $d$ that contains messages received from $d$ neighbours
of the node $v$.

$$receive_A(x(v), m(v), d)$$
The function returns a new local internal state of the node $v$, which becomes a
starting internal state $x(v)$ in the next round.

\subsection{Randomised algorithms}

This subsection will introduce a randomised distributed model of computation
and highlight some of the difference between randomised and deterministic
models. In this section, the focus will be specifically on randomised LOCAL
model as opposed to some other distributed algorithms models.

In a randomised LOCAL model, we have two major differences from the
deterministic LOCAL model. First, the function $init_A(u(v), d)$ becomes
randomised. This means that the initial state $x_0(v) = init_A(u(v), d)$ of a node $v$ is
chosen from a discrete probability distribution of all possible initial states.
Second, the function $receive_A(x(v), m(v), d)$ becomes randomised. This means that,
after all messages have been sent and received for round $i$, a new local internal
state $x_i(v) = receive_A(x(v), m(v), d)$ for a node $v$ is selected from a
discrete probability distribution. Everything else in the model remains exactly
as it is in the deterministic case, that is no changes are made to the $send_A(x(v), d)$
function.

Since probabilities are involved when in randomised setting, it might not be
drectly obvious what it means to "solve" a problem. The major problem is that
since states of the nodes are chosen from a discrete probability distribution,
there is always a probability that the output of the nodes will not be valid.
Therefore, there is often a probability that the problem has not been solved
after a certain number of rounds.

But before we define what it means to solve a problem in a randomised setting, it's
worth noting that there exist two kinds of randomised algorithms: "Monte Carlo" and
"Las Vegas". \emph{Monte Carlo} algorithms always stop after a specified $f(n)$
number of rounds, where $n$ is the number of nodes in the graph, but
it is not guaranteed that the output will a valid one. Monte Carlo algorithm
only guarantee their execution time, and succeed only with probability $p$.
\emph{Las Vegas} algorithms, on the other hand, always produce a correct output,
once all the nodes in a graph have stopped, but the algorithm will stop after a
specified $f(n)$ number of rounds only with some probability $p$. Thus, in some
way, these types of randomised algorithms are complements of each other:
one guarantees correctness of the output but has a chance of going over some
specified execution time, while another has a guarantee on the execution time
but has some probability of producing incorrect output. In the rest of the thesis,
unless explicitely specified otherwise, we assume that the randomised algorithms
are Monte Carlo algorithms.

Finally, we now can define what it means to solve a problem in a randomised context.
When saying that a certain randomised algorithm "solves" a cerrtain problem in
$T(n)$ rounds, unless specified otherwise, we mean that the algorithm stops after
$T(n)$ computational rounds and the nodes output (or to be more precise the nodes
have transitioned to one of the output states) a correct solution "with high
probability" (often denoted as "w.h.p."). Formally, if an algorithm succeeds with
high probability, it means that the algorithm succeds with probability of at least
$1 - 1 / n^c$, where $n$ is a number of nodes in a graph and $c$ is some constant
such that $c > 0$. Moreover, $c$ is a constant that can be freely chosen when 
running the algorithm. To give a simple example, algorithm's running time may
linearly depend on such a constant $c$, then before an algorithm is executed,
one may freely choose the constant so that the larger the constant the longer is
the running time, but also the lower the chance that the produced output will
be incorrect. Notice that $c$ does not need to affect running time, but it often
does since the larger it is the lower is the probability of algorithm's failure,
and decreasing such probability usually comes at the cost of something else - 
often running time. Thus, when saying that something succeeds "with high probability"
we not mean it vaguely but rather refer to a very precise mathematical definition 
given above.

\section{Iterated logarithm}

This section will introduce the iterated logarithm function. The function is
one of the most common complexity results of distributed algorithms and will
appear often in the rest of the document.

The interated logarithm function, denoted as $log^*(x)$ is defined as follows:

\[
    \log^*(x) = \begin{cases}
        0 & \text{ if $x \le 1$}, \\
        1 + \log^*(\log_2 x) & \text{ otherwise}.
    \end{cases}
\]
The function is notable for the fact that it grows extremely slow. For example,

$$log^*(1) = 0$$
$$log^*(2) = 1$$
$$log^*(4) = 2$$
$$log^*(16) = 3$$
$$log^*(65536) = 4$$
$$log^*(2^{65536}) = 5$$

\section{Locally Checkable Labelling problems}

This section will introduce locally checkable labelling problems (from now on referred to as
LCLs), which are the main focus of this thesis. The section will start with
a short historical note and then provide a formal definition of
LCL problems.

As the field of distributed algorithms started to develop, it became clear
that some classes of problems
are of a particular interest to the theoretical research community.
One class of such problems has been first introduced in 1993 by
Moni Naor and Larry Stockmeyer under the name of Locally Checkable 
Labelling (LCL) problems~\cite{Naor1993}.

In short, an LCL problem is a distributed problem that satisfies the 
following criteria~\cite{Naor1993, Suomela2020}.

\begin{itemize}

\item the maximum degree of a graph is a finite number that does not depend on $n$

\item there is a finite number of input labels and the number is not dependent on $n$

\item there is a finite number of output labels and the number does not depend on $n$

\item the correctness of a solution can be checked locally by each individual node. That is
after a solution is produced and all the nodes in the graph have stopped, each node can just
check a radius-$O(1)$ neighbourhood around itself. The solution is valid if and only if each of
the individual neighbourhoods for all the nodes are valid.

\end{itemize}
In the list above, $n$ is the number of nodes in the graph. Finnally, as is easy to see from the definition of LCL
probles
above, there is always a finite representation of any LCL problem. Indeed, one can simply list
all valid local neighbourhoods, list all valid input labels, list all valid output
labels and specify the maximum degree of a graph. Since all of the beforementioned are
of size $O(1)$, it is always possible to have a finite representaion of an LCL.
In practive however, it is in many cases more practical to come up with
a moe concise representation. We will return to the topic of representing LCL
problems later in this thesis.

The study of LCL problems has been one of the major research directions
during the last 6-7 years, with numerous papers published in major
distributed computing conferences
~\cite{Balliu2016, Chang2016, Brandt2017, Chang2017, Fischer2017a, Rozhon2019, Balliu2020-1, Balliu2020-2}.
Currently, the study of LCL problems has reached the stage of maturity with,
for example, the complexity landscape of LCL problems understood
almost entirely ~\cite{Suomela2020, Chang2020a}.

\section{Major LCL problems}

This section will introduce some of the common LCL problems
widely studied in distributed algorithms research community.
Note that all of the problems also exist in the context of 
centralised computing.

\textbf{Vertex coloring} is perhaps one of the most well-known
problems on graphs. Informally, the goal is to color a graph
in such a way that no two adjacent nodes are colored with the
same colors. It is also easy to see tshat the problem is an
LCL problem because it is sufficient and necessary for each node to check
its radius-1 neighbourhood. If each node's radius-1 neighbourhood is
a valid one i.e. none of the neighbours of node $v$ have the same color as $v$
itself, then and only then the output is a valid vertex colorings

Another well-known and widely studied problem on graphs is
\textbf{edge coloring}. Informally, each edge needs to be
colored in such a way that no two adjacent edges are of the same
color. Coloring edges in a graph can still be simulated
with the setting in which only nodes output the labels.
Each node will output a tuple of the size equal to the node's
degree. Each element of a tuple represents a label
that the node outputs on a coresponding edge. The problem
is also an LCL problem, because after a final output is
produced, each node $v$ will need to check its radius-1
neighbourhood to make sure 2 things are in order:
\begin{enumerate}
  \item All edges incident to $v$ have to be of different color.
That is no two elements of node $v$'s output tuple should be the same.

  \item For each edge incident to $v$, a color that node $v$ colored some incident
edge $e$, should be the same as the color that node $u$ colored the edge $e$ with.
Here $e = \{v, u\}$. In other words, because each edge's coloring depends on
output of 2 nodes, such coloring has to be consistent for each edge i.e. the produced
colors need to be the same on both ends of each edge.
\end{enumerate}

An \textbf{independent set} is a problem in which some subset of vertices in a graph
join a set $I$ and some do not. The restriction is that no pair of vertices in the set $I$
can be adjacent nodes. A \textbf{maximal independent set} is a restriction of the
problem in which in addition to what has been stated above, there is an additional
maximality restriction. Maximality restriction forbids cases where a node $v$
is not in the set $I$ but also does not have any adjacent nodes in the set $I$.
In other words, in \textbf{maximal independent set} all nodes either belong
to the set $I$ or they \emph{cannot} join becuase one of their neighbours has
already joined. It is trivial to see that the problem is an LCL as checking local
eighbourhood will suffice to determine if the solution is valid or not.

Finally, a \textbf{maximal matching} is a problem in which nodes need to
produce a matching such that each node is either matched with one of 
its neighbours or it cannot be matched because all of its neighbours are
matched. That is no node can be left unmatched while at least one of its
neighbours is unmatched. It is trivial to see that the problem is an LCL
since each node can check its radius-1 neighbourhood to check whether it
is matched with a neighbour or all neighbours are matched with someone else.
And if the condition of the previous sentence is not satisfied, the output
is invalid.

\section{Recent developments in automated classification of LCL probems}

At this point that the theoretical background has been given, it is
useful to give a background to much more recent developments in the
distributed algorithms research. As the branch of research
studying LCL problems had been reaching maturity, many focused
on the idea of autmated classification of LCL problems. That is
the question of interest is whether we can create meta-algorithms
that would take a description of an LCL problem as its input
and produce the problems round complexity as an output.
This being the core topic of the thesis, it is thus important
to describe all the prior work done in this direction.
In the rest of the
section, we will introduce some of the negative as well as positive results
related to automated classification of LCL problems.

The questions of whether a certain class of LCL problems
can be automatically classified are often referred to
as decidability question. Roughly speaking, a problem is
said to be decidable if there exists an algorithm that,
given the problem, will output its complexity. Such decidability
questions are often studied in relation to the whole
family of graph problems and not to individual instances
of a problem.

First of all, it is important to notice that it has been
proven that in general LCL problems are undecidable.
In fact, already in the cases when the graph family is a grid,
LCL problems cannot be automatically classified~\cite{Brandt2017, Naor1993}.

Nevertheless, many interesting LCL problems are still decidable.
For example, it was known for several years now that LCLs
on paths and cycles are decidable~\cite{Balliu2018, Brandt2017, Naor1993}.
Besides, certain types of LCL problems are also decidable on
trees~\cite{Chang2017}.

But the fact the a problem is decidable in theory does not
always imply that it is possible in practice to
construct an algorith for automatic classification of
such problems. For instance, it is known that even in
the case of paths and cycles, if a node labelling is
given as part of the input, decidability becomes
PSPACE hard~\cite{Balliu2018}.

However, not everything looks that gloomy, and many
interesting and sufficiently broad family of LCL
problems are both decidable and possible to
automatically classify in practice. Indeed a lot of work
has been done attempting to derive practival algorithms
that determine complexity of a given LCL problem,
especially in trees.

Balliu at al. has shown that a complete classification
of binary labelling problems (the problems where a node's
output is allowed to be only one of the two possible labels)
at least in a deterministic setting is possible and moreover
can be done in $O(1)$ time, since computational
complexity of such problems can be simply looked up in a
table in a constant time ~\cite{Balliu2019c}. In addition
to this, the paper also shows tha it is decidable to
classify at least \emph{some} of the randomised binary labelling
LCL problem~\cite{Balliu2019c}.

Building on top of the work outlined in the paper, Rocher
has developed a meta-algorithm that classifies \emph{almost}
all ternary labelling problems on trees~\cite{Rocher2020clas}. Furthermore,
he also implemented the meta-algorithm as a
computer program written in Python programming language~\cite{Rocher2020doc}.
Although the implementation focuses mainly on the case of trees with degree 3,
the techniques described in the manuscript are applicable to ternary problems
on trees of higher degrees as well.

In addition to this, LCL problems on trees and cycles
are fully decidable in polynomial time as was shown by Chang et al.~\cite{Chang2020}.
The algorithm for classifying the problems can be used in practice and has
been implemented in Python programming language by Aalto's
Distributed Algorithms research group~\cite{Tereshchenko2020}.

\section{Language and Structure}

Moreover, the transitions are also used in the paragraph and the
sentence level meaning that all the text is linked together. For example,
the word ``moreover'' here is one way, but of course you should use
variation in the text. Examples of transitional devices (words) and
their use can be found from writing guides, e.g. from the Academic
writing instructions of Aalto
University Language Center
\footnote{http://sana.aalto.fi/awe/ and especially for connecting words 
http://sana.aalto.fi/awe/cohesion/signposts/index.html/} of
Purdue University or Strunk's Elements of
Style\footnote{http://www.bartleby.com/141/}. Remember that footnotes
are additional information, and they are seldom used.  If you refer to a source, you do no
not use footnote. The right command for the references is \emph{cite},
and we will discuss about that later in this Chapter. 

Language Center of Aalto University offers many good courses for
thesis writes. For example, LC-1320 Thesis Writing for Engineers (MSc)
is planned to support writing the master's thesis 
and LC-1310 Academic Communication for MS Students covers both oral
and written language.

The language used in the thesis should be technical (or
scientifical). For example, the abreviations aren't used but all them
are written open (i.e. ``are not''). Since the content itself is often
hard to understand (and explain), the sentences should not be very
long, use complex language with several examples embedded in the same
sentence, and, also, seldom used words and weird euphemism or paraphrases
can make the sentence hard to follow and to read it with only one
time, and making everything even harder to understand all this without
any punctuation marks makes the instructor cry and finally after
trying to correct the language, you will get boomerang, and everyone's
time has just been wasted.

Please use proofreaders before sending even your unfinished version to
the instructor and/or supervisor. You will get better comments when
they do not need first proofread your text. Moreover, they can
consentrate to the content better if the language and spelling
mistakes are not distracting the reading. Several editors have their
own proofreading tools, e.g. ispell in emacs. You can also use
Microsoft Word to proofread your thesis: it can correct also some
grammatical errors and not just misspelled words. You can translate
your latex file to rtf with the \texttt{latex2rtf} command in the
kosh.aalto.fi shell server. Then, the line breaks
will not be problems for the proofreader of Word.

Note also that if you have a section or a subsection, you have to have
at least two of them, or otherwise the section or subsection title is
unnecessary. Same with the paragraphs: you should not have sections
with only one paragraphe, and single sentence paragraph. Furthermore,
always write some text after the title before the next level title.

\section{Finding and referring to sources}

Never ever copy anything into your theses from somebody else's text
(nor your own previously published text). Never. Not even for starting
point to be rewritten later. The risk is that you forgot the copied
text to your thesis and end up to be accused of plagiarism. Plagiarism
is a serious crime in studies and science and can ruin your career
even its beginning. To repeate: never cut and paste text into your
thesis!

\subsection{Finding sources}

All work is based on someone else's work. You should find the relevant
sources of your field and choose the best of them. Also, you should
refer to the original source where a fact has been mentioned first
time. Remember source evaluation (criticism) with all sources you
find.

Good starting points for finding references in computer science are: 
\begin{itemize}
% You can use this command to set the items in the list closer to each other
% (ITEM SEParation, the vertical space between the list items) 
\setlength{\itemsep}{0pt}
\item Aalto library's Computer Science Guide: \url{http://libguides.aalto.fi/computer} 
(in English) and \url{http://libguides.aalto.fi/tietotekniikka} (in Finnish)
\item Finna Portal (Aalto Library): \url{https://aalto.finna.fi/?lng=en-gb} (in English) 
and \url{https://aalto.finna.fi/} (in Finnish)
\item ACM Digital library: \url{http://portal.acm.org/}
\item IEEExplore: \url{http://ieeexplore.ieee.org}
\item ScienceDirect: \url{http://www.sciencedirect.com/}
\item \ldots although Google Scholar (\url{http://scholar.google.com/}) will
find links to most of the articles from the abovementioned sources, if you
search from within the university network
\end{itemize}

Some of the publishers do not offer all the text of the articles
freely, but the library has agreed on the rights to use the whole
text. Thus, you should sometimes use computers in the domain of the
university in order to get the full text. Sometimes the Finna Portal
can also help getting the whole article instead of just the abstract.
The library has also a self-study guide to information retrieval.

Instead of normal Google, use Google Scholar
(\url{http://scholar.google.fi/}). It finds academic publications whereas
normal Google find too much commercial advertisements or otherwise
biased information. Wikipedia articles should be referred to in the master
thesis only very, very seldomly. You can use Wikipedia for understanding
some basics and finding more sources, but often you cannot be sure if
the article is correct and unbiased.

One important part of the sources that you have found is the reference
list. This way you can find the original sources that all the other
research of the field refer. Often you can also find more information
with the name of the researchers that are often referred in the
articles.

\subsection{Sources and reference list}

The main point in referring to sources is to separate your own
thinking and text from that of others. Facts of the research area can
be given without reference, but otherwise you should refer to
sources. This means two things: marking the source in the text where
it has been used, and listing the sources usually in the end of the
thesis in a way that help the reader to find the original source. 
Aalto library has a comprehensive citation guide
.

There are several bibliography styles, meaning how to form the
bibliography in the end of the thesis and how to mark the references
in the text. You should ask from your supervisor or instructors which
style you should use. This thesis template uses the number style that
is often used in software engineering. Here, the bibliography is in
the alphabetical order, not in the order where the sources are
referred, and the sources are marked with numbers in the text. In all
styles, the key idea is to collect as much information of the sources
as is possible in the bibliography, and then let the latex environment take
care of organizing the necessary information to the reference list.

The other bibliographic styles are also used in the CS field. For example, usability
uses the Harvard style where instead of numbers, the reference is
marked into the text with author's name and publishing year. You can
change the bibliography style in the thesis-example.tex file. You get
the normal text reference, e.g. (Haapasalo, 2010), with latex command
\texttt{citet} or the plain \texttt{cite}, and with command
\texttt{citep}, you get the text reference ``Haapasalo (2010)'' that
you can use as subject of a sentence. Next, we tell more about how to mark
the references in the text.

\subsection{Referring to sources}

In addition to the list in the end of the thesis, you have to mark the
source in the text where the source is used. There are three places
for the reference: in a sentence before the period, in the end of a
sentence after the period, or in the end of a paragraph. All of them
have different meaning. The main point is that first you paraphrase
the source using your own words and then mark the source. Next, we
give short examples that are marked with \emph{emphasised text}.

\emph{Haapasalo researched database algorithms
  that allows use of previous versions of the content stored in the
  database.} This kind of marking means that this paragraph (or until
the next reference is given) is based on the source mentioned in the
beginning.  Giving the source you should use only the family name of
the first author of the article, and not give any hints about what is
the type of the article that is referred nor its title.

\emph{B+-trees offers one way to index data that is stored in to a
  database. Multiversion B+-trees (MVBT) offer also a way to restore
  the data from previous versions of the database. Concurrent MVBT
  allows many simultaneous updates to the database that is was not
  possible with MVBT.} When the marking is
after the period, the reference is retrospective: all the paragraph
(or after previous reference marking) is based on the source given in
its end. If the content is very broad, you can start with saying
\emph{According to Haapasalo}, then continue referring the source with
several separate sentences, and in the end put the marking of your
source \emph{ that shows that CMVBT are the
  best. }. 

If your paragraph has several sources, the above mentioned styles are
not proper. The reader of your thesis cannot know which of your
sources give which of the statements. In this case, it is better to
use more finegraded refering where the reference markings that are
embedded in the sentences. For example, \emph{the multiversion B+-tree
  (MVBT) index of Becker et al. allows database
  users to query old versions of the database, but the index is not
  transactional.
  It's successor, the transactional MBVT (TMVBT), allows a single transaction
  running in its own thread or process to update the database concurrently
  with other transactions that only read the
  database. 
  Further development, titled the concurrent MBVT (CMVBT),
  allows several transactions to perform updates to the database at the same
  time}. 
  Here, the references are marked before
  the period in the sentences where they are used. You should never
  but all these sources in the end of the paragraph. Referring several
  source at once should only used when you give a set of examples.

Finally, direct quotes are allowed. However, often you should avoid
them since they do not usually fit in to your text very well. Using
direct quotes has two tricks: quotation marks and the source.  \emph{
  ``Even though deletions in a multiversion index must not physically
  delete the history of the data items, queries and range scans can
  become more efficient, if the leaf pages of the index structure are
  merged to retain optimality.''} Quotes are
hard to make neatly since you should use only as much as needed
without changing the text. Moreover, you often do not really
understand what the author has mentioned with his wordings if you
cannot write the same with your own words. Remember also that never
cut and paste anything without marking the quotation marks right away,
and in general, never cut and paste anything at all!

Sometimes getting the original source can be almost impossible. In an
extremely desperate situation, you can refer with structure \emph{ms
  X~[\ldots] according to mr Y~[\ldots] defined that}, if you find a
source that refers to the original source. Note also that the
reference marking is never used as sentence element (example of how 
\textbf{not} to do it: \emph{ describes
an optimal algorithm for indexing multiversiond databases.}).



















